# Struggle Draft

## From Struggle to Scaffolding: How AI Can Make Learning to Code More Human

Learning to code has always involved a kind of friction. Debugging, tracing logic, figuring out what went wrong and why — these aren't side effects of learning; they're the core of it. But for many students, especially those just starting out, that friction can feel indistinguishable from failure. They wonder if they're cut for it. To seasoned educators, the struggle looks familiar — even necessary. But to a beginner, it often feels like being asked to do push-ups without knowing why, or being told to "wax on, wax off" without seeing the point.

In truth, learning to code shares more with learning a physical skill than many people realize. No one expects to learn how to land a ollie on a skateboard, shoot a perfect basketball free throw, or play a piano sonata without repeated failure. We understand intuitively that physical mastery takes time — and that frustration is part of the path. Programming is no different. Writing (and rewriting) lots of code, getting unstuck from blocks or bugs, and slowly building a mental model of how the machine works — this is the "muscle memory" of computing.  In CS, we have not always done a good job of explaining that this _struggle_ is not a sign of failure, but the very substance of learning.

This kind of challenge, when well-supported, leads to what educators call _productive struggle_. It pushes learners just beyond their current ability, nudging them to problem-solve, receive feedback, and ultimately grow. Productive struggle is where deep learning lives. But when learners are unsupported or alone, struggle quickly becomes frustration — and frustration becomes quitting. For years, this pattern has silently excluded thousands of potential programmers: people who could have learned, but never felt like they belonged.

Generative AI tools promise to change this scenario. They can answer questions instantly, suggest code, fix bugs, and explain complex concepts in plain language. For some, this feels like cheating. For others, it feels like finally having the mentor they always needed. Either way, the presence of these tools is no longer a future to prepare for. The question is no longer _should_ we use AI in education, but _how_.

### The Real Disruption Isn't AI. It's What AI Makes Visible.

Many of the anxieties about generative AI in education stem from the fear that it will eliminate the very struggle that makes learning meaningful. And it might, if used passively. But the truth is: this isn't the first time technology has threatened the struggle.

Powerful tools have been eroding productive struggle for years. Integrated Development Environments (IDEs) auto-complete code, highlight errors before you even hit "Run," and provide real-time debugging tips. Stack Overflow made it easy to copy-paste solutions without understanding the problem. Online tutorials  and videos allow students to follow along step-by-step, replicating code without grappling with the logic. Code snippets, templates, and GitHub boilerplates mean that students often start with something pre-written instead of from scratch. 

Each of these tools made programming more accessible — and each introduced new risks of shallow learning. Educators responded. They created assignments that emphasized design reasoning, added reflection prompts, and evaluated process as much as product. The same must now be done with generative AI.

Because generative AI doesn’t just support coding — it can do much of it. And if we’re not careful, it can encourage a kind of hollow learning: students may submit perfect-looking code they don’t understand, sidestepping the very cognitive work that leads to real skill.

The encouraging news is that AI gives us the chance to redesign how we teach in a way that actually improves learning — not just for the few who would persist through frustration, but for everyone.

### Raising the Bar, Not Lowering It

AI doesn't make struggle disappear; it gives us tools to make struggle more constructive. It lets us scaffold learning in ways that were previously impossible. With AI, students can get instant examples, explanations, and alternatives. They can receive just-in-time feedback tailored to their exact problem. And they can offload boilerplate code to focus on creative or conceptual challenges.

This can free students to focus on higher-order thinking: why does this work, is there a better way, what trade-offs are involved? The goal shifts from "make it run" to "make it make sense."

But this only works if we treat AI not as a substitute for learning, but as a partner in it. That means shifting more responsibility — and more ownership — to the learner. In an AI-powered world, it’s not enough to submit working code. You need to explain it, debug it when it breaks, improve it when it falls short, and adapt it to new contexts. If the AI wrote your first draft, the final version is still yours.

This brings us directly into the realm of motivation and learner identity. Research on learning distinguishes between surface, strategic, and deep learners. Surface learners do the minimum to get by. Strategic learners optimize for rewards. Deep learners seek understanding. In a world where answers are always one prompt away, we must design environments that cultivate deep learners. That means connecting tasks to personal interests, encouraging reflection and iteration, and valuing intrinsic motivation over just performance metrics.

The more students feel in control of their learning — that their work matters, that it reflects their voice, that they can shape it — the more likely they are to engage meaningfully. AI can support that, but only if the surrounding pedagogy demands it.

### Learning Out Loud

To create this kind of learning environment, we don’t need to invent new pedagogies from scratch. Computer science education already has powerful traditions that emphasize process, reflection, and collaboration. Three stand out.

Peer learning emphasizes collaborative construction of knowledge. In CS, this often takes the form of group problem-solving, code reviews, or design discussions. The value is not just in getting help — it’s in explaining, justifying, and comparing approaches. With generative AI, peer learning becomes even more powerful. Students can bring AI-generated code into the discussion, but now they have to answer: does this make sense, why did it suggest that, can we improve it? AI becomes a third collaborator — one whose ideas still need to be evaluated.

Studio-based learning, borrowed from architecture and design, places emphasis on iteration, critique, and public sharing. Students present drafts, receive feedback, and reflect on their decisions. In an AI context, this approach is ideal. Instead of submitting final projects, students show how they worked: what prompts they used, what they changed, what they learned. This demystifies AI use and makes learning visible again. The focus shifts from "Did it work?" to "How did you get there, and why did you make those choices?"

Pair programming involves two students working together on the same problem — one writes code (the "driver"), the other guides (the "navigator"). It requires constant verbal reasoning and slows down impulsive decisions. When AI is involved, pair programming becomes even more reflective. The pair can evaluate AI-generated code together, challenge its suggestions, and decide when to accept or reject help. It keeps learners accountable to each other, not just to the tool.

### This Is a Good Thing

The real promise of AI in education isn't just that it can do things for us — it's that it gives us the chance to rethink how we teach and learn. For too long, computer science has been seen as inaccessible, intimidating, or only for a certain kind of mind. Generative AI can help change that. It can lower the barrier to entry. It can support learners when teachers aren't available. It can make experimentation feel less risky. It can empower students to pursue ideas that interest them.

But this only happens if we make the human side of learning more visible, not less. If we center reflection, ownership, and agency. If we teach students not just to write code, but to think, explain, and care about what they build.

We don't need to protect learners from AI. We need to prepare them to use it wisely, critically, and creatively. And in doing so, we might just make learning to code — and learning itself — more accessible, more personal, and more human than ever before.